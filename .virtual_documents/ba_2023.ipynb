





import re
import datetime as dt
import warnings
import zipfile
import io

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

from scipy import stats
from scipy.stats import chi2_contingency
import statsmodels.api as sm
import statsmodels.formula.api as smf

from sklearn.preprocessing import MinMaxScaler, FunctionTransformer, OrdinalEncoder, StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import RFE, SelectKBest, f_classif, f_regression

from sklearn.model_selection import cross_val_score, cross_val_predict, RandomizedSearchCV
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.svm import LinearSVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier

from sklearn.linear_model import LinearRegression, SGDRegressor, Ridge, Lasso
from sklearn.svm import LinearSVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, VotingRegressor
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor

import tensorflow as tf

from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

from sklearn.metrics import mean_absolute_error





def list_files_in_zip(zip_file_path):
    zip_files = list()
    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
        file_list = zip_ref.namelist()
        for file in file_list:
            zip_files.append(file)
    return zip_files

def read_csv_from_zip(zip_file_path, csv_file_name):
    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
        with zip_ref.open(csv_file_name) as file:
            df = pd.read_csv(io.TextIOWrapper(file))
            return df


zip_file_path = 'ba2023-regression.zip'

ba2023_files = list_files_in_zip(zip_file_path)
ba2023_files


train = read_csv_from_zip(zip_file_path, ba2023_files[1])
train.head()


test = read_csv_from_zip(zip_file_path, ba2023_files[0])
test.head()


sample_sub = read_csv_from_zip(zip_file_path, ba2023_files[3])
sample_sub.head()





train.info()





for col in train.columns:
    print(col, ":", train[col].nunique())


for col in train.columns:
    if train[col].nunique() <= 5:
        print(col, ":", train[col].unique())


train = train.assign(
    date=lambda x: pd.to_datetime(x.date, format='%Y%m%d'),
)





deep_colors = [
    '#2e921b', '#7f1b92', '#4C72B0', '#55A868',
    '#C44E52', '#8172B2', '#CCB974', '#64B5CD'
]


df = train.copy()





df = df.drop(columns=['courier_id', 'tracking_id', 'id', 'group', 'source_tracking_id', 'aoi_id', 'shop_id'])
df.describe(include='all')








df.action_type.value_counts().plot(
    kind='barh', color=deep_colors[1], ylabel='Action Type',
    xlabel='Count', title='Distribution of Action Type'
)
plt.gca().invert_yaxis()
plt.grid(alpha=.3)
plt.show()


fig, ax = plt.subplots(figsize=(8, 6))

pd.crosstab(df.hour, df.action_type).sort_values(by='PICKUP').plot(
    kind='barh', stacked=True, ax=ax, color=(deep_colors[1], deep_colors[2])
)
ax.set_title('Stacked Bar Chart of Hour')
ax.set_ylabel('Hour')
ax.set_xlabel('Count')
ax.legend(title='Action Type', labels=['Delivery', 'Pickup'], loc='upper left', bbox_to_anchor=(1, 1))
plt.grid(alpha=.3)
plt.show()


fig, ax = plt.subplots(figsize=(8, 6))

pd.crosstab(df.wave_index, df.action_type).plot(
    kind='barh', stacked=True, ax=ax, color=(deep_colors[1], deep_colors[2])
)
ax.set_title('Stacked Bar Chart of Wave Index')
ax.set_ylabel('Wave Index')
ax.set_xlabel('Count')
ax.legend(title='Action Type', labels=['Delivery', 'Pickup'], loc='upper left', bbox_to_anchor=(1, 1))
ax.invert_yaxis()
plt.grid(alpha=.3)
plt.show()





fig, ax = plt.subplots(figsize=(8, 6))

pd.crosstab(df.level, df.action_type).plot(
    kind='barh', stacked=True, ax=ax, color=(deep_colors[1], deep_colors[2])
)
ax.set_title('Stacked Bar Chart of Courier Levels')
ax.set_ylabel('Level')
ax.set_xlabel('Count')
ax.legend(title='Action Type', labels=['Delivery', 'Pickup'], loc='upper left', bbox_to_anchor=(1, 1))
plt.grid(alpha=.3)
plt.show()


fig, ax = plt.subplots(figsize=(8, 6))

pd.crosstab(df.max_load, df.action_type).sort_values(by='PICKUP').plot(
    kind='barh', stacked=True, ax=ax, color=(deep_colors[1], deep_colors[2])
)
ax.set_title('Stacked Bar Chart of Max Load')
ax.set_ylabel('Max Load')
ax.set_xlabel('Count')
ax.legend(title='Action Type', labels=['Delivery', 'Pickup'], loc='upper left', bbox_to_anchor=(1, 1))
plt.grid(alpha=.3)
plt.show()


fig, ax = plt.subplots(figsize=(8, 6))

pd.crosstab(df.max_load, df.action_type).plot(
    kind='bar', ax=ax, color=(deep_colors[1], deep_colors[2])
)
ax.set_title('Stacked Bar Chart of Max Load')
ax.set_xlabel('Max Load')
ax.set_ylabel('Count')
ax.legend(title='Action Type', labels=['Delivery', 'Pickup'], loc='upper left', bbox_to_anchor=(1, 1))
plt.grid(alpha=.3)
plt.show()


fig, ax = plt.subplots(figsize=(8, 6))

pd.crosstab(df.weather_grade, df.action_type).sort_values(by='PICKUP').plot(
    kind='barh', stacked=True, ax=ax, color=(deep_colors[1], deep_colors[2])
)
ax.set_title('Stacked Bar Chart of Weather Grade')
ax.set_ylabel('Weather Grade')
ax.set_xlabel('Count')
ax.legend(title='Action Type', labels=['Delivery', 'Pickup'], loc='upper left', bbox_to_anchor=(1, 1))
plt.grid(alpha=.3)
plt.show()


fig, ax = plt.subplots(figsize=(8, 6))

pd.crosstab(df.source_type, df.action_type).sort_values(by='DELIVERY').plot(
    kind='barh', stacked=True, ax=ax, color=(deep_colors[1], deep_colors[2])
)
ax.set_title('Stacked Bar Chart of Source Type')
ax.set_ylabel('Source Type')
ax.set_xlabel('Count')
ax.legend(title='Action Type', labels=['Delivery', 'Pickup'], loc='upper left', bbox_to_anchor=(1, 1))
plt.grid(alpha=.3)
plt.show()





fig, ax = plt.subplots(1, 2, figsize=(10, 6), sharey=True)
sns.histplot(
    data=df, x="courier_wave_start_lng", kde=True,
    color=deep_colors[1], ax=ax[0]
)
sns.histplot(
    data=df, x="courier_wave_start_lat", kde=True,
    color=deep_colors[1], ax=ax[1]
)
ax[0].set_xlabel('Starting Longitude')
ax[1].set_xlabel('Starting Latitude')
ax[0].set_ylabel('Count')
ax[1].set_ylabel('')
ax[0].grid(alpha=.6)
ax[1].grid(alpha=.6)
plt.suptitle('Distribution of Starting Longitude & Latitude', fontsize=18)
plt.show()


fig, ax = plt.subplots(1, 2, figsize=(10, 6), sharey=True)
sns.histplot(
    data=df, x="courier_wave_start_lng", kde=True, ax=ax[0],
    hue='action_type', palette={'DELIVERY': deep_colors[1], 'PICKUP': deep_colors[2]},
    legend=False
)
sns.histplot(
    data=df, x="courier_wave_start_lat", kde=True, ax=ax[1],
    hue='action_type', palette={'DELIVERY': deep_colors[1], 'PICKUP': deep_colors[2]}
)
ax[0].set_xlabel('Starting Longitude')
ax[0].set_ylabel('Count')
ax[0].grid(alpha=.6)

ax[1].set_xlabel('Starting Latitude')
ax[1].set_ylabel('')
ax[1].grid(alpha=.6)
ax[1].legend(title='Action Type', labels=['Delivery', 'Pickup'], loc='upper left', bbox_to_anchor=(1, 1))

plt.suptitle('Distribution of Starting Longitude & Latitude', fontsize=18)
plt.show()


fig, ax = plt.subplots(1, 2, figsize=(10, 6), sharey=True)
sns.histplot(
    data=df, x="source_lng", kde=True, ax=ax[0],
    hue='action_type', palette={'DELIVERY': deep_colors[1], 'PICKUP': deep_colors[2]},
    legend=False
)
sns.histplot(
    data=df, x="source_lat", kde=True, ax=ax[1],
    hue='action_type', palette={'DELIVERY': deep_colors[1], 'PICKUP': deep_colors[2]}
)
ax[0].set_xlabel('Source Longitude')
ax[0].set_ylabel('Count')
ax[0].grid(alpha=.6)

ax[1].set_xlabel('Source Latitude')
ax[1].set_ylabel('')
ax[1].grid(alpha=.6)
ax[1].legend(title='Action Type', labels=['Delivery', 'Pickup'], loc='upper left', bbox_to_anchor=(1, 1))

plt.suptitle('Distribution of Source Longitude & Latitude', fontsize=18)
plt.show()


fig, ax = plt.subplots(1, 2, figsize=(10, 6), sharey=True)
sns.histplot(
    data=df, x="target_lng", kde=True, ax=ax[0],
    hue='action_type', palette={'DELIVERY': deep_colors[1], 'PICKUP': deep_colors[2]},
    legend=False
)
sns.histplot(
    data=df, x="target_lat", kde=True, ax=ax[1],
    hue='action_type', palette={'DELIVERY': deep_colors[1], 'PICKUP': deep_colors[2]}
)
ax[0].set_xlabel('Target Longitude')
ax[0].set_ylabel('Count')
ax[0].grid(alpha=.6)

ax[1].set_xlabel('Target Latitude')
ax[1].set_ylabel('')
ax[1].grid(alpha=.6)
ax[1].legend(title='Action Type', labels=['Delivery', 'Pickup'], loc='upper left', bbox_to_anchor=(1, 1))

plt.suptitle('Distribution of Target Longitude & Latitude', fontsize=18)
plt.show()


fig, ax = plt.subplots(figsize=(10, 6), sharey=True)
sns.histplot(
    data=df, x="speed", kde=True, ax=ax,
    hue='action_type', palette={'DELIVERY': deep_colors[1], 'PICKUP': deep_colors[2]},
)

ax.set_xlabel('Speed')
ax.set_ylabel('Count')
ax.grid(alpha=.6)

ax.legend(title='Action Type', labels=['Delivery', 'Pickup'], loc='upper left', bbox_to_anchor=(1, 1))
ax.set_title('Distribution of Speed')
plt.show()





fig, ax = plt.subplots(figsize=(10, 6), sharey=True)
sns.histplot(
    data=df, x="grid_distance", kde=True, ax=ax,
    hue='action_type', palette={'DELIVERY': deep_colors[1], 'PICKUP': deep_colors[2]},
)

ax.set_xlabel('Grid Distance')
ax.set_ylabel('Count')
ax.grid(alpha=.6)

ax.legend(title='Action Type', labels=['Delivery', 'Pickup'], loc='upper left', bbox_to_anchor=(1, 1))
ax.set_title('Distribution of Grid Distance')
plt.show()





fig, ax = plt.subplots(figsize=(10, 6), sharey=True)
sns.histplot(
    data=df, x="expected_use_time", kde=True, ax=ax,
    hue='action_type', palette={'DELIVERY': deep_colors[1], 'PICKUP': deep_colors[2]},
)

ax.set_xlabel('Expected Use Time')
ax.set_ylabel('Count')
ax.grid(alpha=.6)

ax.legend(title='Action Type', labels=['Delivery', 'Pickup'], loc='upper left', bbox_to_anchor=(1, 1))
ax.set_title('Distribution of Expected Use Time')
plt.show()


fig, ax = plt.subplots(figsize=(10, 6), sharey=True)
sns.histplot(
    data=df, x="urgency", kde=True, ax=ax,
    hue='action_type', palette={'DELIVERY': deep_colors[1], 'PICKUP': deep_colors[2]},
)

ax.set_xlabel('Urgency')
ax.set_ylabel('Count')
ax.grid(alpha=.6)

ax.legend(title='Action Type', labels=['Delivery', 'Pickup'], loc='upper left', bbox_to_anchor=(1, 1))
ax.set_title('Distribution of Urgency')
plt.show()





outlier_cols = ['grid_distance', 'urgency']
df[outlier_cols].describe()


sns.boxplot(
    data=df, x='action_type', y=outlier_cols[0],
    hue='action_type', palette={'DELIVERY': deep_colors[1], 'PICKUP': deep_colors[2]}
)

plt.xlabel('Action Type')
plt.ylabel('Grid Distance')
plt.title('Box Plot of Grid Distance with Action Type')
plt.legend(title='Action Type', labels=['Delivery', 'Pickup'], loc='upper left', bbox_to_anchor=(1, 1))
plt.show()


sns.boxplot(
    data=df, x='action_type', y=outlier_cols[1],
    hue='action_type', palette={'DELIVERY': deep_colors[1], 'PICKUP': deep_colors[2]}
)

plt.xlabel('Action Type')
plt.ylabel('Urgency')
plt.legend(title='Action Type', labels=['Delivery', 'Pickup'], loc='upper left', bbox_to_anchor=(1, 1))
plt.title('Box Plot of Urgency with Action Type')

plt.show()





def remove_outliers(data, columns, k=1.5):
    for column in columns:
        q1 = data[column].quantile(0.25)
        q3 = data[column].quantile(0.75)
        iqr = q3 - q1
        lower_bound = q1 - k * iqr
        upper_bound = q3 + k * iqr
        data = data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]
    return data

no_outliers = remove_outliers(df[outlier_cols + ['action_type']], outlier_cols)
no_outliers.shape


sns.boxplot(
    data=no_outliers, x='action_type', y=outlier_cols[0],
    hue='action_type', palette={'DELIVERY': deep_colors[1], 'PICKUP': deep_colors[2]}
)

plt.xlabel('Action Type')
plt.ylabel('Grid Distance')
plt.title('Box Plot of Grid Distance with Action Type - No Outliers')
plt.legend(title='Action Type', labels=['Delivery', 'Pickup'], loc='upper left', bbox_to_anchor=(1, 1))
plt.show()


sns.boxplot(
    data=no_outliers, x='action_type', y=outlier_cols[1],
    hue='action_type', palette={'DELIVERY': deep_colors[1], 'PICKUP': deep_colors[2]}
)

plt.xlabel('Action Type')
plt.ylabel('Urgency')
plt.title('Box Plot of Urgency with Action Type - No Outliers')
plt.legend(title='Action Type', labels=['Delivery', 'Pickup'], loc='upper left', bbox_to_anchor=(1, 1))
plt.show()





# Adding a constant to shift data
constant = 1  # Or any suitable constant to shift values
transformed_data, lambda_val = stats.boxcox(df[outlier_cols[0]] + constant)
df['transformed_grid_distance'] = transformed_data

plt.figure(figsize=(8, 5)) 
plt.subplot(1, 2, 1)
sns.boxplot(y=df.grid_distance, color=deep_colors[1])
plt.title('Grid Distance')
plt.ylabel('Values')

plt.subplot(1, 2, 2)
sns.boxplot(y=transformed_data, color=deep_colors[1])
plt.title('Box-Cox Transformed Grid Distance')
plt.ylabel('Transformed Values')

plt.show()


outlier_cols[1]


scaler = MinMaxScaler()
urgency_scaled = scaler.fit_transform(df[outlier_cols[1]].values.reshape(-1, 1))

constant = 1
transformed_data, lambda_val = stats.boxcox(urgency_scaled.reshape(-1) + constant)
df['transformed_urgency'] = transformed_data

plt.figure(figsize=(8, 6))
plt.subplot(1, 2, 1)
sns.boxplot(y=df.grid_distance, color=deep_colors[1])
plt.title('Urgency')
plt.ylabel('Values')

plt.subplot(1, 2, 2)
sns.boxplot(y=transformed_data, color=deep_colors[1])
plt.title('Scaled & Box-Cox Transformed Urgency')
plt.ylabel('Transformed Values')

plt.show()





df.columns








contingency_table = pd.crosstab(df.hour, df.action_type)
display(contingency_table)

chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
print('P-value: ', p_val)
if p_val <= 0.05:
    print('For action type, we reject the null hypothesis.')
else:
    print('For action type, we fail to reject the null hypothesis.')


model = smf.ols('expected_use_time ~ C(hour)', data=df).fit()

# Perform ANOVA
anova_table = sm.stats.anova_lm(model, typ=2)
print(anova_table)

p_val = anova_table.iloc[0, 3]
print('\nP-value: ', p_val)
if p_val <= 0.05:
    print('For expected use time, we reject the null hypothesis.')
else:
    print('For expected use time, we fail to reject the null hypothesis.')





contingency_table = pd.crosstab(df.wave_index, df.action_type)
display(contingency_table)

chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
print('P-value: ', p_val)
if p_val <= 0.05:
    print('For action type, we reject the null hypothesis.')
else:
    print('For action type, we fail to reject the null hypothesis.')


model = smf.ols('expected_use_time ~ C(wave_index)', data=df).fit()

# Perform ANOVA
anova_table = sm.stats.anova_lm(model, typ=2)
print(anova_table)

p_val = anova_table.iloc[0, 3]
print('\nP-value: ', p_val)
if p_val <= 0.05:
    print('For expected use time, we reject the null hypothesis.')
else:
    print('For expected use time, we fail to reject the null hypothesis.')





contingency_table = pd.crosstab(df.level, df.action_type)
display(contingency_table)

chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
print('P-value: ', p_val)
if p_val <= 0.05:
    print('For action type, we reject the null hypothesis.')
else:
    print('For action type, we fail to reject the null hypothesis.')


model = smf.ols('expected_use_time ~ C(level)', data=df).fit()

# Perform ANOVA
anova_table = sm.stats.anova_lm(model, typ=2)
print(anova_table)

p_val = anova_table.iloc[0, 3]
print('\nP-value: ', p_val)
if p_val <= 0.05:
    print('For expected use time, we reject the null hypothesis.')
else:
    print('For expected use time, we fail to reject the null hypothesis.')





contingency_table = pd.crosstab(df.max_load, df.action_type)
display(contingency_table)

chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
print('P-value: ', p_val)
if p_val <= 0.05:
    print('For action type, we reject the null hypothesis.')
else:
    print('For action type, we fail to reject the null hypothesis.')


model = smf.ols('expected_use_time ~ C(max_load)', data=df).fit()

# Perform ANOVA
anova_table = sm.stats.anova_lm(model, typ=2)
print(anova_table)

p_val = anova_table.iloc[0, 3]
print('\nP-value: ', p_val)
if p_val <= 0.05:
    print('For expected use time, we reject the null hypothesis.')
else:
    print('For expected use time, we fail to reject the null hypothesis.')





contingency_table = pd.crosstab(df.weather_grade, df.action_type)
display(contingency_table)

chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
print('P-value: ', p_val)
if p_val <= 0.05:
    print('For action type, we reject the null hypothesis.')
else:
    print('For action type, we fail to reject the null hypothesis.')


model = smf.ols('expected_use_time ~ C(weather_grade)', data=df).fit()

# Perform ANOVA
anova_table = sm.stats.anova_lm(model, typ=2)
print(anova_table)

p_val = anova_table.iloc[0, 3]
print('\nP-value: ', p_val)
if p_val <= 0.05:
    print('For expected use time, we reject the null hypothesis.')
else:
    print('For expected use time, we fail to reject the null hypothesis.')





contingency_table = pd.crosstab(df.source_type, df.action_type)
display(contingency_table)

chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
print('P-value: ', p_val)
if p_val <= 0.05:
    print('For action type, we reject the null hypothesis.')
else:
    print('For action type, we fail to reject the null hypothesis.')


model = smf.ols('expected_use_time ~ C(source_type)', data=df).fit()

# Perform ANOVA
anova_table = sm.stats.anova_lm(model, typ=2)
print(anova_table)

p_val = anova_table.iloc[0, 3]
print('\nP-value: ', p_val)
if p_val <= 0.05:
    print('For expected use time, we reject the null hypothesis.')
else:
    print('For expected use time, we fail to reject the null hypothesis.')





contingency_table = pd.crosstab(df.date, df.action_type)
display(contingency_table)

chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
print('P-value: ', p_val)
if p_val <= 0.05:
    print('For action type, we reject the null hypothesis.')
else:
    print('For action type, we fail to reject the null hypothesis.')


model = smf.ols('expected_use_time ~ C(date)', data=df).fit()

# Perform ANOVA
anova_table = sm.stats.anova_lm(model, typ=2)
print(anova_table)

p_val = anova_table.iloc[0, 3]
print('\nP-value: ', p_val)
if p_val <= 0.05:
    print('For expected use time, we reject the null hypothesis.')
else:
    print('For expected use time, we fail to reject the null hypothesis.')





contingency_table = pd.crosstab(df.source_lng, df.action_type)
display(contingency_table)

chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
print('P-value: ', p_val)
if p_val <= 0.05:
    print('For action type, we reject the null hypothesis.')
else:
    print('For action type, we fail to reject the null hypothesis.')


model = smf.ols('expected_use_time ~ source_lng', data=df).fit()

# Perform ANOVA
anova_table = sm.stats.anova_lm(model, typ=2)
print(anova_table)

p_val = anova_table.iloc[0, 3]
print('\nP-value: ', p_val)
if p_val <= 0.05:
    print('For expected use time, we reject the null hypothesis.')
else:
    print('For expected use time, we fail to reject the null hypothesis.')





contingency_table = pd.crosstab(df.target_lng, df.action_type)
display(contingency_table)

chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
print('P-value: ', p_val)
if p_val <= 0.05:
    print('For action type, we reject the null hypothesis.')
else:
    print('For action type, we fail to reject the null hypothesis.')


model = smf.ols('expected_use_time ~ target_lng', data=df).fit()

# Perform ANOVA
anova_table = sm.stats.anova_lm(model, typ=2)
print(anova_table)

p_val = anova_table.iloc[0, 3]
print('\nP-value: ', p_val)
if p_val <= 0.05:
    print('For expected use time, we reject the null hypothesis.')
else:
    print('For expected use time, we fail to reject the null hypothesis.')





contingency_table = pd.crosstab(df.courier_wave_start_lng, df.action_type)
display(contingency_table)

chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
print('P-value: ', p_val)
if p_val <= 0.05:
    print('For action type, we reject the null hypothesis.')
else:
    print('For action type, we fail to reject the null hypothesis.')


model = smf.ols('expected_use_time ~ courier_wave_start_lng', data=df).fit()

# Perform ANOVA
anova_table = sm.stats.anova_lm(model, typ=2)
print(anova_table)

p_val = anova_table.iloc[0, 3]
print('\nP-value: ', p_val)
if p_val <= 0.05:
    print('For expected use time, we reject the null hypothesis.')
else:
    print('For expected use time, we fail to reject the null hypothesis.')





contingency_table = pd.crosstab(df.grid_distance, df.action_type)
display(contingency_table)

chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
print('P-value: ', p_val)
if p_val <= 0.05:
    print('For action type, we reject the null hypothesis.')
else:
    print('For action type, we fail to reject the null hypothesis.')


model = smf.ols('expected_use_time ~ grid_distance', data=df).fit()

# Perform ANOVA
anova_table = sm.stats.anova_lm(model, typ=2)
print(anova_table)

p_val = anova_table.iloc[0, 3]
print('\nP-value: ', p_val)
if p_val <= 0.05:
    print('For expected use time, we reject the null hypothesis.')
else:
    print('For expected use time, we fail to reject the null hypothesis.')





contingency_table = pd.crosstab(df.urgency, df.action_type)
display(contingency_table)

chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
print('P-value: ', p_val)
if p_val <= 0.05:
    print('For action type, we reject the null hypothesis.')
else:
    print('For action type, we fail to reject the null hypothesis.')


model = smf.ols('expected_use_time ~ urgency', data=df).fit()

# Perform ANOVA
anova_table = sm.stats.anova_lm(model, typ=2)
print(anova_table)

p_val = anova_table.iloc[0, 3]
print('\nP-value: ', p_val)
if p_val <= 0.05:
    print('For expected use time, we reject the null hypothesis.')
else:
    print('For expected use time, we fail to reject the null hypothesis.')








train = read_csv_from_zip(zip_file_path, ba2023_files[1])


def initial_cleaning(data, date_col, action_type_col=None):
    drop_cols = ['courier_id', 'tracking_id', 'id', 'group', 'source_tracking_id', 'aoi_id', 'shop_id', 'courier_wave_start_lng', 'courier_wave_start_lat']
    data = data.drop(drop_cols, axis=1)
    
    if action_type_col:
        data[date_col] = pd.to_datetime(data[date_col], format='%Y%m%d')
        data[action_type_col] = data[action_type_col].apply(lambda y: 1 if y.lower() == 'delivery' else 0)
    else:
        data[date_col] = pd.to_datetime(data[date_col], format='%Y%m%d')
    data['year'] = data[date_col].dt.year
    data['month'] = data[date_col].dt.month
    data['day'] = data[date_col].dt.day
    
    data = data.drop(columns=date_col)
    
    return data


def boxcox_transform(x):
    x = x.values
    transformed, _ = stats.boxcox(x)
    return transformed.reshape(-1, 1)

def boxcox_transform_with_constant(x, constant):
    if isinstance(x, pd.DataFrame):
        x = x.values.reshape(-1)
    else:
        x = x.reshape(-1)
    transformed, _ = stats.boxcox(x + constant)
    return transformed.reshape(-1, 1)


train = initial_cleaning(train, 'date', action_type_col='action_type')
train.info()


# Pipeline no Transformations

ordinal_encoder = OrdinalEncoder(categories=[['Normal Weather', 'Slightly Bad Weather', 'Bad Weather', 'Very Bad Weather']])
one_hot_encoder = OneHotEncoder()

preprocessor = ColumnTransformer(
    transformers=[
        ('ordinal_encoder', ordinal_encoder, ['weather_grade']),
        ('one_hot', one_hot_encoder, ['source_type']),
    ],
    remainder='passthrough' 
)

pipeline = Pipeline([('preprocessor', preprocessor),])
pipeline.fit(train)
transformed_data = pipeline.transform(train)

cols = ['wave_index', 'action_type', 'level', 'speed', 'max_load', 'source_lng', 'source_lat', 'target_lng', 'target_lat', 'grid_distance', 'expected_use_time', 'urgency', 'hour', 'year', 'month', 'day']
column_names = list()
for name, transformer, columns in pipeline.named_steps['preprocessor'].transformers:
    if name == 'one_hot':
        column_names.extend(pipeline.named_steps['preprocessor'].named_transformers_[name].get_feature_names_out(columns))
    else:
        column_names.extend(columns)
column_names.extend(cols)

train2 = pd.DataFrame(transformed_data, columns=column_names)
train2 = train2.apply(pd.to_numeric, errors='ignore')
train2.columns = train2.columns.str.lower()
train2.info()


# Creating a pipeline for feature transformations
constant_urgency = 1
constant_distance = 1

column_transformer = ColumnTransformer([
    ('ordinal_encoder', OrdinalEncoder(
        categories=[['Normal Weather', 'Slightly Bad Weather', 'Bad Weather', 'Very Bad Weather']]
    ), ['weather_grade']),
    ('one_hot', OneHotEncoder(), ['source_type']),
    ('min_max_scaling_and_boxcox', Pipeline([
        ('min_max_scaler', MinMaxScaler()),
        ('boxcox_transform', FunctionTransformer(func=boxcox_transform_with_constant, kw_args={'constant': constant_urgency}))
    ]), ['urgency']),
    ('boxcox_transform_feature4', FunctionTransformer(func=boxcox_transform_with_constant, kw_args={'constant': constant_distance}), ['grid_distance'])
], remainder='passthrough')

transformed_data = column_transformer.fit_transform(train)

transformed_feature_names = ['weather_grade_encoded'] + ['source_type_'  + cat.lower() for cat in train['source_type'].unique()]
transformed_feature_names.extend(['urgency_transformed', 'grid_distance_transformed', 'wave_index', 'action_type', 'level', 'speed', 'max_load', 'source_lng', 'source_lat', 'target_lng', 'target_lat', 'expected_use_time', 'hour', 'year', 'month', 'day'])

transformed_df = pd.DataFrame(transformed_data, columns=transformed_feature_names)
transformed_df = pd.concat([transformed_df, train[['urgency', 'grid_distance']]], axis=1)
transformed_df = transformed_df.apply(pd.to_numeric, errors='ignore')
transformed_df.head()








X = train2.drop(columns=['action_type', 'expected_use_time'])
y1 = train2['action_type']
y2 = train2['expected_use_time']

X_train, X_test, y1_train, y1_test, y2_train, y2_test = train_test_split(X, y1, y2, test_size=0.2, random_state=2)
X_train, X_val, y1_train, y1_val, y2_train, y2_val = train_test_split(X_train, y1_train, y2_train, test_size=0.2, random_state=3)

print(X_train.shape, X_test.shape, X_val.shape)





def remove_outliers(data, columns, k=1.5):
    for column in columns:
        q1 = data[column].quantile(0.25)
        q3 = data[column].quantile(0.75)
        iqr = q3 - q1
        lower_bound = q1 - k * iqr
        upper_bound = q3 + k * iqr
        data = data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]
    return data

outlier_cols = ['grid_distance', 'urgency']
no_outliers = remove_outliers(train2, outlier_cols)
no_outliers.shape


Xno = no_outliers.drop(columns=['action_type', 'expected_use_time'])
y1no = no_outliers['action_type']
y2no = no_outliers['expected_use_time']

Xno_train, Xno_test, y1no_train, y1no_test, y2no_train, y2no_test = train_test_split(Xno, y1no, y2no, test_size=0.2, random_state=2)
Xno_train, Xno_val, y1no_train, y1no_val, y2no_train, y2no_val = train_test_split(Xno_train, y1no_train, y2no_train, test_size=0.2, random_state=3)

print(Xno_train.shape, Xno_test.shape, Xno_val.shape)





Xtr = transformed_df.drop(columns=['action_type', 'expected_use_time', 'grid_distance', 'urgency'])
y1tr = transformed_df['action_type']
y2tr = transformed_df['expected_use_time']

Xtr_train, Xtr_test, y1tr_train, y1tr_test, y2tr_train, y2tr_test = train_test_split(Xtr, y1, y2, test_size=0.2, random_state=2)
Xtr_train, Xtr_val, y1tr_train, y1tr_val, y2tr_train, y2tr_val = train_test_split(Xtr_train, y1tr_train, y2tr_train, test_size=0.2, random_state=3)

print(Xtr_train.shape, Xtr_test.shape, Xtr_val.shape)





# create binary outcome variable
data = Xtr_train.copy()

# fit logistic regression model
X = data[['urgency_transformed', 'grid_distance_transformed']]
y = y1tr_train
X = sm.add_constant(X) # add intercept term
model = sm.Logit(y, X).fit()

# print model summary
print(model.summary())


correlation_matrix = pd.concat([X_train.drop(columns=['year', 'month']), y1_train], axis=1).corr()

plt.figure(figsize=(15, 12))
sns.heatmap(correlation_matrix, annot=True, cmap='YlGnBu', fmt='.2f')
plt.title('Correlation Matrix Heatmap - Original Data')
plt.show()


correlation_matrix = pd.concat([Xno_train.drop(columns=['year', 'month']), y1no_train], axis=1).corr()

plt.figure(figsize=(15, 12))
sns.heatmap(correlation_matrix, annot=True, cmap='YlGnBu', fmt='.2f')
plt.title('Correlation Matrix Heatmap - No Outliers')
plt.show()





correlation_matrix = pd.concat([Xtr_train.drop(columns=['year', 'month']), y1tr_train], axis=1).corr()

plt.figure(figsize=(15, 12))
sns.heatmap(correlation_matrix, annot=True, cmap='YlGnBu', fmt='.2f')
plt.title('Correlation Matrix Heatmap - Transformed Outliers')
plt.show()





scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

Xno_train_scaled = scaler.fit_transform(Xno_train)
Xno_val_scaled = scaler.transform(Xno_val)
Xno_test_scaled = scaler.transform(Xno_test)

Xtr_train_scaled = scaler.fit_transform(Xtr_train)
Xtr_val_scaled = scaler.transform(Xtr_val)
Xtr_test_scaled = scaler.transform(Xtr_test)


lr = LogisticRegression(max_iter=2000)
sgd_clf = SGDClassifier(max_iter=2000)
linear_svc = LinearSVC(max_iter=2000, dual='auto')
# svc = SVC(max_iter=2000)
knn_clf = KNeighborsClassifier(n_neighbors=5)
tree_clf = DecisionTreeClassifier(max_depth=4, min_samples_leaf=5)
rf_clf = RandomForestClassifier(max_depth=4, min_samples_leaf=5)
et_clf = ExtraTreesClassifier(max_depth=4, min_samples_leaf=5)
gb_clf = GradientBoostingClassifier(max_depth=4, min_samples_leaf=5)
xgb_clf = XGBClassifier()


def simple_classifier_cross_val(
        model, X_train, y_train
):
    ## Helper function to train a classifier with cross-validation
    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    formatted_scores = [f'{score:.2f}' for score in scores]
    formatted_mean_score = f'{scores.mean():.2f}'

    print("Accuracy scores for each fold:", formatted_scores)
    print("Mean Accuracy score:", formatted_mean_score)





model_names = ['LogisticRegression', 'SGDClassifier', 'LinearSVC', 'KNeighborsClassifier','DecisionTreeClassifier', 'RandomForestClassifier', 'ExtraTreesClassifier', 'GradientBoostingClassifier', 'XGBClassifier']
models = [lr, sgd_clf, linear_svc, knn_clf, tree_clf, rf_clf, et_clf, gb_clf, xgb_clf]

with warnings.catch_warnings():
    warnings.simplefilter("ignore")
    for model, model_name in zip(models, model_names):
        if model_name in ['LogisticRegression', 'SGDClassifier', 'LinearSVC', 'KNeighborsClassifier']:
            print(model_name)
            simple_classifier_cross_val(model, X_train_scaled, y1_train)
            print('\n')
        else:
            print(model_name)
            simple_classifier_cross_val(model, X_train, y1_train)
            print('\n')


tf.keras.backend.clear_session()
tf.random.set_seed(42)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=[X_train_scaled.shape[1],]))
for layer in range(100):
    model.add(tf.keras.layers.Dense(100, activation="relu",
                                    kernel_initializer="he_normal"))

model.add(tf.keras.layers.Dense(10, activation="softmax"))

model.compile(loss="sparse_categorical_crossentropy",
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,
                                                 beta_1=0.9, beta_2=0.999),
              metrics=["accuracy"])

history = model.fit(X_train_scaled, y1_train, epochs=5,
                    validation_data=(X_val_scaled, y1_val))





# Build the model
model = XGBClassifier()
model.fit(X_train, y1_train)
feature_importance = model.feature_importances_

feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importance})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plot feature importances
plt.figure(figsize=(8, 6))
plt.barh(
    feature_importance_df['Feature'], feature_importance_df['Importance'], color=deep_colors[1],
)
plt.xlabel('Importance')
plt.title('XGB Feature Importances')
plt.gca().invert_yaxis()
plt.grid(alpha=.3)
plt.show()



# Build the model
model = LogisticRegression(max_iter=1000)
rfe = RFE(model, n_features_to_select=3) # select top 3 features
fit = rfe.fit(X_train_scaled, y1_train)

print("Selected Features: ", X_train.columns[fit.support_])

inverted_ranking = len(rfe.ranking_) - rfe.ranking_ + 1
feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': inverted_ranking})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plot feature importances
plt.figure(figsize=(8, 6))
plt.barh(
    feature_importance_df['Feature'], feature_importance_df['Importance'], color=deep_colors[1]
)
plt.xlabel('Importance')
plt.title('RFE Feature Importances')
plt.gca().invert_yaxis()
plt.grid(alpha=.3)
plt.show()


# Create an instance of SelectKBest with chi2 scoring function
selector = SelectKBest(f_classif, k=3)

# Fit the selector on the training set
X_train_selected = selector.fit_transform(X_train, y1_train)

# Print the selected features
print("Selected Features: ", X_train.columns[selector.get_support()])

feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': selector.scores_})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(8, 6))
plt.barh(
    feature_importance_df['Feature'], feature_importance_df['Importance'], color=deep_colors[1]
)
plt.xlabel('Importance')
plt.title('SelectKBest Feature Importances')
plt.gca().invert_yaxis()
plt.show()








lr_reg = LinearRegression()
sgd_reg = SGDRegressor(max_iter=2000)
linear_svr = LinearSVR(max_iter=2000, dual='auto')
ridge_reg = Ridge()
lasso_reg = Lasso()
knn_reg = KNeighborsRegressor(n_neighbors=5)
tree_reg = DecisionTreeRegressor(max_depth=4, min_samples_leaf=5)
rf_reg = RandomForestRegressor(max_depth=4, min_samples_leaf=5)
et_reg = ExtraTreesRegressor(max_depth=4, min_samples_leaf=5)
gb_reg = GradientBoostingRegressor(max_depth=4, min_samples_leaf=5)
xgb_reg = XGBRegressor()


def simple_regressor_cross_val(model, X_train, y_train):
    ## Helper function to train a classifier with cross-validation
    scores = -cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')
    formatted_scores = [f'{score:.2f}' for score in scores]
    formatted_mean_score = f'{scores.mean():.2f}'

    print("MAE scores for each fold:", formatted_scores)
    print("Mean MAE score:", formatted_mean_score)


model_names = ['LinearRegression', 'SGDRegressor', 'LinearSVR', 'RidgeRegression', 'LassoRegression', 'KNeighborsRegressor','DecisionTreeRegressor', 'RandomForestRegressor', 'ExtraTreesRegressor', 'GradientBoostingRegressor', 'XGBRegressor']
models = [lr_reg, sgd_reg, linear_svr, ridge_reg, lasso_reg, knn_reg, tree_reg, rf_reg, et_reg, gb_reg, xgb_reg]

with warnings.catch_warnings():
    warnings.simplefilter("ignore")
    for model, model_name in zip(models, model_names):
        if model_name in ['LinearRegression', 'SGDRegressor', 'LinearSVR', 'RidgeRegression', 'LassoRegression', 'KNeighborsRegressor']:
            print(model_name)
            simple_regressor_cross_val(model, X_train_scaled, y2_train)
            print('\n')
        else:
            print(model_name)
            simple_regressor_cross_val(model, X_train, y2_train)
            print('\n')





# Build the model
model = XGBRegressor()
model.fit(X_train, y2_train)
feature_importance = model.feature_importances_

feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importance})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plot feature importances
plt.figure(figsize=(8, 6))
plt.barh(
    feature_importance_df['Feature'], feature_importance_df['Importance'], color=deep_colors[1],
)
plt.xlabel('Importance')
plt.title('XGB Feature Importances')
plt.gca().invert_yaxis()
plt.grid(alpha=.3)
plt.show()



# Build the model
model = LinearSVR(max_iter=2000, dual='auto')
rfe = RFE(model, n_features_to_select=3) # select top 3 features
fit = rfe.fit(X_train_scaled, y2_train)

print("Selected Features: ", X_train.columns[fit.support_])

inverted_ranking = len(rfe.ranking_) - rfe.ranking_ + 1
feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': inverted_ranking})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plot feature importances
plt.figure(figsize=(8, 6))
plt.barh(
    feature_importance_df['Feature'], feature_importance_df['Importance'], color=deep_colors[1]
)
plt.xlabel('Importance')
plt.title('RFE Feature Importances')
plt.gca().invert_yaxis()
plt.grid(alpha=.3)
plt.show()


# Create an instance of SelectKBest with chi2 scoring function
selector = SelectKBest(f_regression, k=3)

# Fit the selector on the training set
X_train_selected = selector.fit_transform(X_train, y2_train)

# Print the selected features
print("Selected Features: ", X_train.columns[selector.get_support()])

feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': selector.scores_})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(8, 6))
plt.barh(
    feature_importance_df['Feature'], feature_importance_df['Importance'], color=deep_colors[1]
)
plt.xlabel('Importance')
plt.title('SelectKBest Feature Importances')
plt.gca().invert_yaxis()
plt.show()








model_names = ['LogisticRegression', 'SGDClassifier', 'LinearSVC', 'KNeighborsClassifier','DecisionTreeClassifier', 'RandomForestClassifier', 'ExtraTreesClassifier', 'GradientBoostingClassifier', 'XGBClassifier']
models = [lr, sgd_clf, linear_svc, knn_clf, tree_clf, rf_clf, et_clf, gb_clf, xgb_clf]

with warnings.catch_warnings():
    warnings.simplefilter("ignore")
    for model, model_name in zip(models, model_names):
        if model_name in ['LogisticRegression', 'SGDClassifier', 'LinearSVC', 'KNeighborsClassifier']:
            print(model_name)
            simple_classifier_cross_val(model, Xno_train_scaled, y1no_train)
            print('\n')
        else:
            print(model_name)
            simple_classifier_cross_val(model, Xno_train, y1no_train)
            print('\n')





# Build the model
model = XGBClassifier()
model.fit(Xno_train, y1no_train)
feature_importance = model.feature_importances_

feature_importance_df = pd.DataFrame({'Feature': Xno_train.columns, 'Importance': feature_importance})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plot feature importances
plt.figure(figsize=(8, 6))
plt.barh(
    feature_importance_df['Feature'], feature_importance_df['Importance'], color=deep_colors[1],
)
plt.xlabel('Importance')
plt.title('XGB Feature Importances')
plt.gca().invert_yaxis()
plt.grid(alpha=.3)
plt.show()



# Build the model
model = LogisticRegression(max_iter=1000)
rfe = RFE(model, n_features_to_select=3) # select top 3 features
fit = rfe.fit(Xno_train_scaled, y1no_train)

print("Selected Features: ", Xno_train.columns[fit.support_])

inverted_ranking = len(rfe.ranking_) - rfe.ranking_ + 1
feature_importance_df = pd.DataFrame({'Feature': Xno_train.columns, 'Importance': inverted_ranking})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plot feature importances
plt.figure(figsize=(8, 6))
plt.barh(
    feature_importance_df['Feature'], feature_importance_df['Importance'], color=deep_colors[1]
)
plt.xlabel('Importance')
plt.title('RFE Feature Importances')
plt.gca().invert_yaxis()
plt.grid(alpha=.3)
plt.show()


# Create an instance of SelectKBest with chi2 scoring function
selector = SelectKBest(f_classif, k=3)

# Fit the selector on the training set
X_train_selected = selector.fit_transform(Xno_train, y1no_train)

# Print the selected features
print("Selected Features: ", Xno_train.columns[selector.get_support()])

feature_importance_df = pd.DataFrame({'Feature': Xno_train.columns, 'Importance': selector.scores_})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(8, 6))
plt.barh(
    feature_importance_df['Feature'], feature_importance_df['Importance'], color=deep_colors[1]
)
plt.xlabel('Importance')
plt.title('SelectKBest Feature Importances')
plt.gca().invert_yaxis()
plt.show()





model_names = ['LinearRegression', 'SGDRegressor', 'LinearSVR', 'Ridge', 'Lasso', 'KNeighborsRegressor','DecisionTreeRegressor', 'RandomForestRegressor', 'ExtraTreesRegressor', 'GradientBoostingRegressor', 'XGBRegressor']
models = [lr_reg, sgd_reg, linear_svr, ridge_reg, lasso_reg, knn_reg, tree_reg, rf_reg, et_reg, gb_reg, xgb_reg]

with warnings.catch_warnings():
    warnings.simplefilter("ignore")
    for model, model_name in zip(models, model_names):
        if model_name in ['LinearRegression', 'SGDRegressor', 'LinearSVR', 'Ridge', 'Lasso', 'KNeighborsRegressor']:
            print(model_name)
            simple_regressor_cross_val(model, Xno_train_scaled, y2no_train)
            print('\n')
        else:
            print(model_name)
            simple_regressor_cross_val(model, Xno_train, y2no_train)
            print('\n')





# Build the model
model = XGBRegressor()
model.fit(Xno_train, y2no_train)
feature_importance = model.feature_importances_

feature_importance_df = pd.DataFrame({'Feature': Xno_train.columns, 'Importance': feature_importance})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plot feature importances
plt.figure(figsize=(8, 6))
plt.barh(
    feature_importance_df['Feature'], feature_importance_df['Importance'], color=deep_colors[1],
)
plt.xlabel('Importance')
plt.title('XGB Feature Importances')
plt.gca().invert_yaxis()
plt.grid(alpha=.3)
plt.show()



# Build the model
model = LinearSVR(max_iter=1000, dual='auto')
rfe = RFE(model, n_features_to_select=3) # select top 3 features
fit = rfe.fit(Xno_train_scaled, y2no_train)

print("Selected Features: ", Xno_train.columns[fit.support_])

inverted_ranking = len(rfe.ranking_) - rfe.ranking_ + 1
feature_importance_df = pd.DataFrame({'Feature': Xno_train.columns, 'Importance': inverted_ranking})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plot feature importances
plt.figure(figsize=(8, 6))
plt.barh(
    feature_importance_df['Feature'], feature_importance_df['Importance'], color=deep_colors[1]
)
plt.xlabel('Importance')
plt.title('RFE Feature Importances')
plt.gca().invert_yaxis()
plt.grid(alpha=.3)
plt.show()


# Create an instance of SelectKBest with chi2 scoring function
selector = SelectKBest(f_regression, k=3)

# Fit the selector on the training set
X_train_selected = selector.fit_transform(Xno_train, y2no_train)

# Print the selected features
print("Selected Features: ", Xno_train.columns[selector.get_support()])

feature_importance_df = pd.DataFrame({'Feature': Xno_train.columns, 'Importance': selector.scores_})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(8, 6))
plt.barh(
    feature_importance_df['Feature'], feature_importance_df['Importance'], color=deep_colors[1]
)
plt.xlabel('Importance')
plt.title('SelectKBest Feature Importances')
plt.gca().invert_yaxis()
plt.show()





model_names = ['LogisticRegression', 'SGDClassifier', 'LinearSVC', 'KNeighborsClassifier','DecisionTreeClassifier', 'RandomForestClassifier', 'ExtraTreesClassifier', 'GradientBoostingClassifier', 'XGBClassifier']
models = [lr, sgd_clf, linear_svc, knn_clf, tree_clf, rf_clf, et_clf, gb_clf, xgb_clf]

with warnings.catch_warnings():
    warnings.simplefilter("ignore")
    for model, model_name in zip(models, model_names):
        if model_name in ['LogisticRegression', 'SGDClassifier', 'LinearSVC', 'KNeighborsClassifier']:
            print(model_name)
            simple_classifier_cross_val(model, Xtr_train_scaled, y1tr_train)
            print('\n')
        else:
            print(model_name)
            simple_classifier_cross_val(model, Xtr_train, y1tr_train)
            print('\n')





# Build the model
model = XGBClassifier()
model.fit(Xtr_train, y1tr_train)
feature_importance = model.feature_importances_

feature_importance_df = pd.DataFrame({'Feature': Xtr_train.columns, 'Importance': feature_importance})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plot feature importances
plt.figure(figsize=(8, 6))
plt.barh(
    feature_importance_df['Feature'], feature_importance_df['Importance'], color=deep_colors[1],
)
plt.xlabel('Importance')
plt.title('XGB Feature Importances')
plt.gca().invert_yaxis()
plt.grid(alpha=.3)
plt.show()



# Build the model
model = LogisticRegression(max_iter=1000)
rfe = RFE(model, n_features_to_select=3) # select top 3 features
fit = rfe.fit(Xtr_train_scaled, y1tr_train)

print("Selected Features: ", Xtr_train.columns[fit.support_])

inverted_ranking = len(rfe.ranking_) - rfe.ranking_ + 1
feature_importance_df = pd.DataFrame({'Feature': Xtr_train.columns, 'Importance': inverted_ranking})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plot feature importances
plt.figure(figsize=(8, 6))
plt.barh(
    feature_importance_df['Feature'], feature_importance_df['Importance'], color=deep_colors[1]
)
plt.xlabel('Importance')
plt.title('RFE Feature Importances')
plt.gca().invert_yaxis()
plt.grid(alpha=.3)
plt.show()


# Create an instance of SelectKBest with chi2 scoring function
selector = SelectKBest(f_classif, k=3)

# Fit the selector on the training set
X_train_selected = selector.fit_transform(Xtr_train, y1tr_train)

# Print the selected features
print("Selected Features: ", Xtr_train.columns[selector.get_support()])

feature_importance_df = pd.DataFrame({'Feature': Xtr_train.columns, 'Importance': selector.scores_})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(8, 6))
plt.barh(
    feature_importance_df['Feature'], feature_importance_df['Importance'], color=deep_colors[1]
)
plt.xlabel('Importance')
plt.title('SelectKBest Feature Importances')
plt.gca().invert_yaxis()
plt.show()


model_names = ['LinearRegression', 'SGDRegressor', 'LinearSVR', 'Ridge', 'Lasso', 'KNeighborsRegressor','DecisionTreeRegressor', 'RandomForestRegressor', 'ExtraTreesRegressor', 'GradientBoostingRegressor', 'XGBRegressor']
models = [lr_reg, sgd_reg, linear_svr, ridge_reg, lasso_reg, knn_reg, tree_reg, rf_reg, et_reg, gb_reg, xgb_reg]

with warnings.catch_warnings():
    warnings.simplefilter("ignore")
    for model, model_name in zip(models, model_names):
        if model_name in ['LinearRegression', 'SGDRegressor', 'LinearSVR', 'Ridge', 'Lasso', 'KNeighborsRegressor']:
            print(model_name)
            simple_regressor_cross_val(model, Xtr_train_scaled, y2tr_train)
            print('\n')
        else:
            print(model_name)
            simple_regressor_cross_val(model, Xtr_train, y2tr_train)
            print('\n')





# Build the model
model = XGBRegressor()
model.fit(Xtr_train, y2tr_train)
feature_importance = model.feature_importances_

feature_importance_df = pd.DataFrame({'Feature': Xtr_train.columns, 'Importance': feature_importance})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plot feature importances
plt.figure(figsize=(8, 6))
plt.barh(
    feature_importance_df['Feature'], feature_importance_df['Importance'], color=deep_colors[1],
)
plt.xlabel('Importance')
plt.title('XGB Feature Importances')
plt.gca().invert_yaxis()
plt.grid(alpha=.3)
plt.show()



# Build the model
model = LinearSVR(max_iter=2000, dual='auto')
rfe = RFE(model, n_features_to_select=3) # select top 3 features
fit = rfe.fit(Xtr_train_scaled, y2tr_train)

print("Selected Features: ", Xtr_train.columns[fit.support_])

inverted_ranking = len(rfe.ranking_) - rfe.ranking_ + 1
feature_importance_df = pd.DataFrame({'Feature': Xtr_train.columns, 'Importance': inverted_ranking})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plot feature importances
plt.figure(figsize=(8, 6))
plt.barh(
    feature_importance_df['Feature'], feature_importance_df['Importance'], color=deep_colors[1]
)
plt.xlabel('Importance')
plt.title('RFE Feature Importances')
plt.gca().invert_yaxis()
plt.grid(alpha=.3)
plt.show()


# Create an instance of SelectKBest with chi2 scoring function
selector = SelectKBest(f_regression, k=3)

# Fit the selector on the training set
X_train_selected = selector.fit_transform(Xtr_train, y2tr_train)

# Print the selected features
print("Selected Features: ", Xtr_train.columns[selector.get_support()])

feature_importance_df = pd.DataFrame({'Feature': Xtr_train.columns, 'Importance': selector.scores_})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(8, 6))
plt.barh(
    feature_importance_df['Feature'], feature_importance_df['Importance'], color=deep_colors[1]
)
plt.xlabel('Importance')
plt.title('SelectKBest Feature Importances')
plt.gca().invert_yaxis()
plt.show()





param_grid = {
    'learning_rate': [0.1, 0.2, 0.3],
    'max_depth': [3, 4, 5],
    'n_estimators': [100, 200, 300],
    'colsample_bytree': [0.6, 0.8],
    'subsample': [0.6, 0.8],
}

# Initialize XGBClassifier
xgb_clf = XGBClassifier()

# grid_search = GridSearchCV(xgb_clf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
random_search = RandomizedSearchCV(xgb_clf, param_distributions=param_grid, cv=5, scoring='accuracy', n_jobs=-1, n_iter=50, verbose=2)

# grid_search.fit(X_train, y1_train)
random_search.fit(X_train, y1_train)

# best_params = grid_search.best_params_
best_params = random_search.best_params_
best_estimator = random_search.best_estimator_
best_score = random_search.best_score_

print(f'best params: {best_params}')
print(f'\nbest estimator: {best_estimator}')
print(f'\nbest score: {best_score}')



param_grid = {
    'learning_rate': [0.1, 0.2, 0.3],
    'max_depth': [3, 4, 5],
    'n_estimators': [100, 200, 300],
    'colsample_bytree': [0.6, 0.8],
    'subsample': [0.6, 0.8],
}

xgb_reg = XGBRegressor()
random_search = RandomizedSearchCV(xgb_reg, param_distributions=param_grid, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1, n_iter=50, verbose=2)
random_search.fit(X_train, y2_train)

best_params = random_search.best_params_
best_estimator = random_search.best_estimator_
best_score = random_search.best_score_

print(f'best params: {best_params}')
print(f'\nbest estimator: {best_estimator}')
print(f'\nbest score: {best_score}')





model = XGBClassifier(
    subsample=.8, n_estimators=300, max_depth=5, 
    learning_rate=.3, colsample_bytree=.8
)
model.fit(X_train, y1_train)
y1_pred = model.predict(X_val)

print(classification_report(y1_val, y1_pred))


y1_pred_proba = model.predict_proba(X_val)[:, 1]  # Probabilities for class 1
threshold = 0.5 # Choose a threshold value (e.g., 0.5 by default)
y1_pred_adjusted = (y1_pred_proba >= threshold).astype(int)

print(classification_report(y1_val, y1_pred_adjusted))


model = XGBRegressor(
    subsample=.8, n_estimators=300, max_depth=5,
    learning_rate=.2, colsample_bytree=.8
)
model.fit(X_train, y2_train)
y2_pred = model.predict(X_val)

print(mean_absolute_error(y2_val, y2_pred))


# Predicting Expected Use Time by including actual action type values
X_val2 = pd.concat([X_val, y1_val], axis=1)
X_test2 = pd.concat([X_test, y1_test], axis=1)

model = XGBRegressor(
    subsample=.8, n_estimators=300, max_depth=5,
    learning_rate=.2, colsample_bytree=.8
)
model.fit(X_val2, y2_val)
y2_pred = model.predict(X_test2)

print(mean_absolute_error(y2_test, y2_pred))


# Predicting Expected Use Time by including predicted action type values
X_val2 = X_val.copy()
X_val2['action_type'] = y1_pred
X_test2 = pd.concat([X_test, y1_test], axis=1)

model = XGBRegressor(
    subsample=.8, n_estimators=300, max_depth=5,
    learning_rate=.2, colsample_bytree=.8
)
model.fit(X_val2, y2_val)
y2_pred = model.predict(X_test2)

print(mean_absolute_error(y2_test, y2_pred))





rel_cols = ['source_type_assign', 'source_type_delivery', 'source_type_pickup', 'grid_distance', 'urgency', 'source_lng', 'source_lat', 'target_lat', 'target_lng', 'hour', 'day', 'speed']

model = XGBClassifier(
    subsample=.8, n_estimators=300, max_depth=5,
    learning_rate=.3, colsample_bytree=.8
)
model.fit(X_train[rel_cols], y1_train)
y1_pred = model.predict(X_val[rel_cols])

print(classification_report(y1_val, y1_pred))


y1_pred_proba = model.predict_proba(X_val[rel_cols])[:, 1]  # Probabilities for class 1
threshold = 0.5 # Choose a threshold value (e.g., 0.5 by default)
y1_pred_adjusted = (y1_pred_proba >= threshold).astype(int)

print(classification_report(y1_val, y1_pred_adjusted))


model = XGBRegressor(
    subsample=.8, n_estimators=300, max_depth=5,
    learning_rate=.2, colsample_bytree=.8
)
model.fit(X_train[rel_cols], y2_train)
y2_pred = model.predict(X_val[rel_cols])

print(mean_absolute_error(y2_val, y2_pred))


# Predicting Expected Use Time by including actual action type values
X_val2 = pd.concat([X_val, y1_val], axis=1)
X_test2 = pd.concat([X_test, y1_test], axis=1)

model = XGBRegressor(
    subsample=.8, n_estimators=300, max_depth=5,
    learning_rate=.2, colsample_bytree=.8
)
model.fit(X_val2[rel_cols], y2_val)
y2_pred = model.predict(X_test2[rel_cols])

print(mean_absolute_error(y2_test, y2_pred))


# Predicting Expected Use Time by including predicted action type values
X_val2 = X_val.copy()
X_val2['action_type'] = y1_pred
X_test2 = pd.concat([X_test, y1_test], axis=1)

model = XGBRegressor(
    subsample=.8, n_estimators=300, max_depth=5,
    learning_rate=.2, colsample_bytree=.8
)
model.fit(X_val2[rel_cols], y2_val)
y2_pred = model.predict(X_test2[rel_cols])

print(mean_absolute_error(y2_test, y2_pred))





def initial_cleaning(data, date_col, action_type_col=None):
    drop_cols = ['courier_id', 'tracking_id', 'id', 'group', 'source_tracking_id', 'aoi_id', 'shop_id', 'courier_wave_start_lng', 'courier_wave_start_lat']
    data = data.drop(drop_cols, axis=1)

    if action_type_col:
        data[date_col] = pd.to_datetime(data[date_col], format='%Y%m%d')
        data[action_type_col] = data[action_type_col].apply(lambda y: 1 if y.lower() == 'delivery' else 0)
    else:
        data[date_col] = pd.to_datetime(data[date_col], format='%Y%m%d')
    data['year'] = data[date_col].dt.year
    data['month'] = data[date_col].dt.month
    data['day'] = data[date_col].dt.day

    data = data.drop(columns=date_col)

    return data



train = read_csv_from_zip(zip_file_path, ba2023_files[1])
train = initial_cleaning(train, 'date', action_type_col='action_type')

ordinal_encoder = OrdinalEncoder(categories=[['Normal Weather', 'Slightly Bad Weather', 'Bad Weather', 'Very Bad Weather']])
one_hot_encoder = OneHotEncoder()

preprocessor = ColumnTransformer(
    transformers=[
        ('ordinal_encoder', ordinal_encoder, ['weather_grade']),
        ('one_hot', one_hot_encoder, ['source_type']),
    ],
    remainder='passthrough'
)
pipeline = Pipeline([('preprocessor', preprocessor),])
pipeline.fit(train)
transformed_data = pipeline.transform(train)

cols = [
    'wave_index', 'action_type', 'level', 'speed', 'max_load', 'source_lng', 'source_lat', 'target_lng', 'target_lat', 'grid_distance', 'expected_use_time', 'urgency', 'hour', 'year', 'month', 'day'
]
column_names = list()
for name, transformer, columns in pipeline.named_steps['preprocessor'].transformers:
    if name == 'one_hot':
        column_names.extend(pipeline.named_steps['preprocessor'].named_transformers_[name].get_feature_names_out(columns))
    else:
        column_names.extend(columns)
column_names.extend(cols)

train2 = pd.DataFrame(transformed_data, columns=column_names)
train2 = train2.apply(pd.to_numeric, errors='ignore')
train2.columns = train2.columns.str.lower()


X = train2.drop(columns=['action_type', 'expected_use_time'])
y1 = train2['action_type']
y2 = train2['expected_use_time']


test = read_csv_from_zip(zip_file_path, ba2023_files[0])
test = initial_cleaning(test, 'date')
test['source_type'] = test['source_type'].apply(lambda x: test['source_type'].mode().iloc[0] if x == '2.10E+18' else x)

ordinal_encoder = OrdinalEncoder(categories=[['Normal Weather', 'Slightly Bad Weather', 'Bad Weather', 'Very Bad Weather']])
one_hot_encoder = OneHotEncoder()

preprocessor = ColumnTransformer(
    transformers=[
        ('ordinal_encoder', ordinal_encoder, ['weather_grade']),
        ('one_hot', one_hot_encoder, ['source_type']),
    ],
    remainder='passthrough'
)
pipeline = Pipeline([('preprocessor', preprocessor),])
pipeline.fit(test)
transformed_data = pipeline.transform(test)

cols = [
    'wave_index', 'level', 'speed', 'max_load', 'source_lng', 'source_lat', 'target_lng', 'target_lat', 'grid_distance', 'urgency', 'hour', 'year', 'month', 'day'
]
column_names = list()
for name, transformer, columns in pipeline.named_steps['preprocessor'].transformers:
    if name == 'one_hot':
        column_names.extend(pipeline.named_steps['preprocessor'].named_transformers_[name].get_feature_names_out(columns))
    else:
        column_names.extend(columns)
column_names.extend(cols)

test = pd.DataFrame(transformed_data, columns=column_names)
test = test.apply(pd.to_numeric, errors='ignore')
test.columns = test.columns.str.lower()


model = XGBClassifier(
    subsample=.8, n_estimators=300, max_depth=5,
    learning_rate=.3, colsample_bytree=.8
)
model.fit(X, y1)
y1_pred = model.predict(test)


y1_pred = pd.DataFrame({'action_type': y1_pred})
y1_pred = y1_pred.reset_index()
y1_pred = y1_pred.rename(columns={'index': 'order'})
y1_pred.to_csv('action_type_pred.csv', index=False)


model2 = XGBRegressor(
    subsample=.8, n_estimators=300, max_depth=5,
    learning_rate=.3, colsample_bytree=.8
)
model2.fit(X, y2)
y2_pred = model2.predict(test)


y2_pred = pd.DataFrame({'expected_use_time': y2_pred})
y2_pred = y2_pred.reset_index() 
y2_pred = y2_pred.rename(columns={'index': 'order'})
y2_pred.to_csv('expected_use_time_pred.csv', index=False)


model3 = XGBRegressor(
    subsample=.8, n_estimators=300, max_depth=5,
    learning_rate=.3, colsample_bytree=.8
)
X2 = X.copy()
X2['action_type'] = y1
test2 = test.copy()
test2['action_type'] = y1_pred.action_type

model3.fit(X2, y2)
y2_pred2 = model3.predict(test2)


y2_pred2 = pd.DataFrame({'expected_use_time': y2_pred2})
y2_pred2 = y2_pred2.reset_index() 
y2_pred2 = y2_pred2.rename(columns={'index': 'order'})
y2_pred2.to_csv('expected_use_time_pred2.csv', index=False)


model = XGBClassifier(
    subsample=.8, n_estimators=300, max_depth=5,
    learning_rate=.3, colsample_bytree=.8
)
model.fit(X[rel_cols], y1)
y1_pred = model.predict(test[rel_cols])


model3 = XGBRegressor(
    subsample=.8, n_estimators=300, max_depth=5,
    learning_rate=.3, colsample_bytree=.8
)
X2 = X.copy()
X2['action_type'] = y1
test2 = test.copy()
test2['action_type'] = y1_pred

model3.fit(X2[rel_cols], y2)
y2_pred2 = model3.predict(test2[rel_cols])


y2_pred2 = pd.DataFrame({'expected_use_time': y2_pred2})
y2_pred2 = y2_pred2.reset_index()
y2_pred2 = y2_pred2.rename(columns={'index': 'order'})
y2_pred2.to_csv('expected_use_time_pred2_relcols.csv', index=False)



